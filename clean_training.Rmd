---
title: "Sean: the HU sentimenent analysis algorithm"
subtitle: "Data-cleaning training data"
output: html_notebook
---

## Summary
Team Data & Analytics is starting project Sean: develop an algorithm that performs sentiment analysis on text with the aim of providing a sentiment score, specifically aimed at HU student-related data (STudent Information Point requests, open-ended questions from student questionnaires, HU twitter, etc.). We aim to develop a product that can be re-used (and further developed) within the HU for any purpose and goal, and will become available through GitHub.

# 1. Load R-libraries
```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
```

# 2. Inladen sean trainingfile 
```{r Read training data}
training <- read_csv2(file = "~/researchdrive/M21033303_DenA (Projectfolder)/DA_Onderzoek/2022sean/data_in/Topdesk_datasample10000_tm20220801_desc.csv", col_names = FALSE)
colnames(training) <- c('id', 'melding', 'verzoek', 'actie')
head(training,10)
```

# 3. Data cleaning of request text
```{r Clean requests by subject string} 
# Select only requests (verzoek column) + convert all text to lowercase
training_clean <- tolower(training$verzoek)

# Remember how many characters each string has
n_chars1 <- nchar(training_clean)

# Find the string 'onderwerp:' (subject) in each message for splitting
ind_onderwerp <- str_locate(training_clean, "onderwerp:")[,1]

# Select message from onderwerp onwards
training_clean <- substr(training_clean, ind_onderwerp, n_chars1)

# Drop messages that did not have a 'onderwerp:' string (out-of-scope)
training_clean <- training_clean[!is.na(training_clean)]
```

```{r Visualize cleaning}
# Remove NA values from vector
n_chars1 <- n_chars1[!is.na(ind_onderwerp)]

# Remember how many characters each string has
n_chars2 <- nchar(training_clean)

# Number of stripped characters histogram
hist((n_chars1 - n_chars2), main = paste("Number of stripped characters (average is:", round(mean(n_chars1 - n_chars2), 1), "chars)"), xlim = range(1:1000), xlab = "# characters stripped", ylab = "Occurrence", breaks = 100)
mean(n_chars1 - n_chars2)
```
It seems most messages have between 100-150 characters cleaned, while on average a message has 138 characters stripped. 

# 4. Export cleaned training data
```{r}
save(training_clean, #gecleande trainingsdata
     file = "~/researchdrive/M21033303_DenA (Projectfolder)/DA_Onderzoek/2022sean/data_in/trainingdata"
     )
```

